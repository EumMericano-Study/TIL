# AI 
머신러닝의 영역은 다양한 이론들이 존재하고 독립적 이거나 유기적으로 연결되어있다.

> 기호주의  /  연결주의  /  진화주의  /  베이스주의  /  유추주의  /  ...
  
이 중 딥러닝에 관심이 생겨 딥러닝의 기초가 되는 연결주의에 대해 알아보았다.  
  
> 뉴턴이 만유인력의 법칙을 발견하기전, 갈릴레오 갈릴레이가 망원경을 통해 태양계 일부 행성들의 위성을 발견하였다.  
> 증기기관을 통해 산업혁명이 일어났지만, 열역학법칙은 한참 후에서야 정리되었다.  
> 인류가 전자를 발견하기 전, 에디슨은 전구를 개발하고 발전소까지 만들었다. 

  
딥러닝은 위의 사례들과 비슷한 상황이다.
왜 잘되는지 모르는 상황이지만, 컴퓨터는 코드(알고리즘)를 스스로 만들어 좋은 성능을 내고 있다.
이 딥러닝이 어떻게 작동하는지 알기 위해서는 신경망에 대한 선행지식이 필요하다.


## 신경망
> 우리의 두뇌는 어떻게 학습하는가?

### 신경망 (NN : Neural Network)

**시작**
최초의 시작은 뇌과학의 연결주의이다. (*연결주의 : 지식이란 신경세포 사이의 연결에 있다.)

사람의 뇌는 흔히 1000억개의 뉴런의 연결이라고 한다. 
이는 일직선으로 연결했을때, 지구와 달의 사이의 거리와 비슷한 길이이다.


<img src=./image/1_EarthnMoon.jpg>

이 뉴런들의 연결은 미지의 영역이다. 
이 연결에 대한 가설 중 하나가 **헵의 규칙**이다. 


**헵의 규칙**이란 
생물학적인 신경 시스템은 프로그래밍처럼 정해진 input값에 대한 output을 출력하는 것이 아니고, 
지속적인 input 값들이 입력되고, 저장되며 서로 유기적으로 작동하여 계속적인 학습으로 이끌어 낸다는 의미이다.
즉, 뇌에 아주 오래전에 입력된 데이터라 할지라도 현재의 지식에 영향을 미친다는 뜻이다.
<img src=./image/neuron.png>


 기존의 기호주의자 학습에서는 기호와 개념사이에 1:1 대응을 이루었다.
 하지만, 연결주의에서는 한가지의 개념이 여러곳에 흩어져있고,
 단순한 정보를 표현하는데에도 수많은 뉴런들이 사용된다.

>기호주의 : 뉴런의 1:1대응적인 연결  /  
>input : 사각형, 바퀴 4개, 사람보다 큰 크기   /  
>output : "자동차" 
>
>연결주의 : 뉴런의  유기적인 연결  /  
>input : 시각적 데이터  /  
>output : 대상 물체가 "자동차"일 것이라는 추론


이를 프로그래밍에 적용한것이 **인공신경망**이다
#
### 인공신경망 ( ANN : Artificial Neural Network)


>기호주의 : input에 대한 output의 Mapping  /  
>input : 사각형, 바퀴 4개, 사람보다 큰 크기   /  
>output : "자동차" 
>
>연결주의 : 각 Node들의 연결  /  
>input : 시각적 데이터  /  
>output : 대상 물체가 "자동차"일 것이라는 추론

**인공신경망 : 다중 퍼셉트론의 예)**

<img src=./image/ANN.png>


우리는 그간 역인역법의 전제에서 목표 결론에 이르기 위해 필요한 규칙을 한 번에 한 단계씩 파악했다. **인공신경망**에서는 이를 동시에 학습한다.

사실, 인공신경망은 완전히 최신화된 이론이 아니다. 
1940년대 맥클럭-피츠 모델을 출발로 오랜기간 연구 컴퓨터 사이언스의 되어온 한 분야일 뿐이다.
다만, 과거의 컴퓨터 성능으로 인한 한계때문에 침체기를 맞이하여 뭍혀져 있다가, 
구글의 딥마인드팀에 의해 다시 빛을 보고 있다.


**퍼셉트론**
퍼셉트론은 데이터가 선형적으로 분리될 수 있을 때 적합한 알고리즘이다.  

<img src=./image/perceptron.png>

위 그림처럼 2가지의 기준을 통한 기준선을 세워 선을 초과하는지, 미달되는지에 대해 비교한다.

이는 확고한 기준선이 있다면 데이터를 정렬할 수 있는 좋은 알고리즘이 될 수 있으나,
일정 영역을 관통하는 XOR문제에서는 힘을 쓸 수 없다.

<img src=./image/percepXOR.png>

이 XOR에 관련된 문제를 해결하는 방법은 다중 layer를 두어 해결하는 방법이다.

<img src=./image/mulper.png>

 Input = [100, 200, 300] ,   hidden = [-1, -2, -3],  output = [0.03, 0.05, -0.92]
 
위와 같은 예시로 벡터값들과 가중치 값들의 내적값을 구하여 output을 뽑아내어
XOR논리 문제를 해결할 수 있다.


* 891개의 트레이닝 데이터를 가진 딥러닝

<img src=./image/kerascode.png>
<img src=./image/keras.png>
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyNjk4NjUzOTMsMTE0ODc3MjAyMSwyMD
M3MzY5MzkyLDEzMzY4NjIzMzksMTYxMjAxNTYyMiwtMzA2MjUw
NzUzLC03Mzg5NjA4MTEsMTUxNjIyNjg4MCwtMTM2NTU0NDIsMj
U3OTEyMjE3LC05Nzc1MjcyMjEsLTE4NzAzMzg4MzMsLTIwMDcz
ODI5NzYsLTYwNjY3MTI0MiwtMzY1MTUxMDMzXX0=
-->