# AI 
머신러닝의 영역은 다양한 이론들이 존재하고 독립적,  유기적으로 연결되어있다.

> 기호주의  /  연결주의  /  진화주의  /  베이스주의  /  유추주의  /  ...
  
  
> 뉴턴이 만유인력의 법칙을 발견하기전, 갈릴레오 갈릴레이가 망원경을 통해 태양계 일부 행성들의 위성을 발견하였다.  
> 증기기관을 통해 산업혁명이 일어났지만, 열역학법칙은 한참 후에서야 정리되었다.  
> 인류가 전자를 발견하기 전, 에디슨은 전구를 개발하고 발전소까지 만들었다. 

  
딥러닝은 위의 사례들과 비슷한 상황이다.
왜 잘되는지 모르는 상황이지만, 컴퓨터는 코드(알고리즘)를 스스로 만들어 좋은 성능을 내고 있다.
이 딥러닝이 어떻게 작동하는지 알기 위해서는 신경망에 대한 선행지식이 필요하다.
이에, 딥러닝의 기초가 되는 **연결주의**에 대해 알아보았다.  

## 신경망
> 우리의 두뇌는 어떻게 학습하는가?					
> 사람의 학습방법과, 기계의 학습방법은 무슨 차이가 있는가?		
> `추상화`

### 신경망 (NN : Neural Network)

**시작**
최초의 시작은 뇌과학의 연결주의이다. (*연결주의 : 지식이란 신경세포 사이의 연결에 있다.)

사람의 뇌는 흔히 1000억개의 뉴런의 연결이라고 한다. 
이는 일직선으로 연결했을때, 지구와 달의 사이의 거리와 비슷한 길이이다.


<img src=./image/1_EarthnMoon.jpg>

이 뉴런들의 연결은 미지의 영역이다. 
이 연결에 대한 가설 중 하나가 **헵의 규칙**이다. 


**헵의 규칙**이란 
생물학적인 신경 시스템은 프로그래밍처럼 정해진 input값에 대한 output을 출력하는 것이 아니고, 
지속적인 input 값들이 입력되고, 저장되며 서로 유기적으로 작동하여 계속적인 학습으로 이끌어 낸다는 의미이다.
즉, 뇌에 아주 오래전에 입력된 데이터라 할지라도 현재의 지식에 영향을 미친다는 뜻이다.
<img src=./image/neuron.png>


 기존의 기호주의자 학습에서는 기호와 개념사이에 1:1 대응을 이루었다.
 하지만, 연결주의에서는 한가지의 개념이 여러곳에 흩어져있고,
 단순한 정보를 표현하는데에도 수많은 뉴런들이 사용된다.

|                |기호주의                         |연결주의                         |
|----------------|-------------------------------|-----------------------------|
|연결 형태|뉴런의 1:1대응적인 연결     |뉴런의  유기적인 연결           |
|input          |사각형, 바퀴 4개, 사람보다 큰 크기   |시각적 데이터            |
|output          |자동차|대상 물체가 '자동차'일 것이라는 추론|

이를 프로그래밍에 적용한것이 **인공신경망**이다

## 인공신경망 ( ANN : Artificial Neural Network)


|                |기호주의 프로그래밍                        |연결주의 프로그래밍                     |
|----------------|-------------------------------|-----------------------------|
|연결 형태| input에 대한 output의 Mapping      | 각 Node들의 연결           |
|input          |사각형, 바퀴 4개, 사람보다 큰 크기   |시각적 데이터            |
|output          |자동차|대상 물체가 '자동차'일 것이라는 추론|

**인공신경망 : 다중 퍼셉트론의 예)**

<img src=./image/ANN.png>


우리는 그간 역인역법의 전제에서 목표 결론에 이르기 위해 필요한 규칙을 한 번에 한 단계씩 파악했다. **인공신경망**에서는 이를 동시에 학습한다.

사실, 인공신경망은 완전히 최신화된 이론이 아니다. 
1940년대 맥클럭-피츠 모델을 출발로 오랜기간 연구 컴퓨터 사이언스의 되어온 한 분야일 뿐이다.
다만, 과거의 컴퓨터 성능으로 인한 한계때문에 침체기를 맞이하여 뭍혀져 있다가, 
구글의 딥마인드팀에 의해 다시 빛을 보고 있다.


**퍼셉트론**
퍼셉트론은 데이터가 선형적으로 분리될 수 있을 때 적합한 알고리즘이다.  

<img src=./image/perceptron.png>

위 그림처럼 2가지의 기준을 통한 기준선을 세워 선을 초과하는지, 미달되는지에 대해 비교한다.

이는 확고한 기준선이 있다면 데이터를 정렬할 수 있는 좋은 알고리즘이 될 수 있으나,
일정 영역을 관통하는 XOR문제에서는 힘을 쓸 수 없다.

<img src=./image/percepXOR.png>

이 XOR에 관련된 문제를 해결하는 방법은 다중 layer를 두어 해결하는 방법이다.


 Input = [100, 200, 300] ,   hidden = [-1, -2, -3, 1],  output = [0.03, -0.92]

<img src=./image/mulper.png>

 
위와 같은 예시로 벡터값들과 가중치 값들의 내적값을 구하여 output을 뽑아내어
XOR논리 문제를 해결할 수 있다.


과거 hidden layer의 갯수가 증가할때는 일정량까지 증가했다가, 
더 많은 layer들을 거치며 자발적인 오류 발생과 성능저하를 일으켰다.

하지만 현대에 들어 하드웨어, 통신기술의 비약적인 발전을 통하여 수없이 많은 layer들을 만들어 딥러닝을 구현했을때, 
해당 분야에 따라 사람들이 흉내낼 수 없을 정도의 정확도를 기록할 수 있게되었다.


* 891개의 트레이닝 데이터의  머신러닝 & 딥러닝 그래프

<img src=./image/kerascode.png>
<img src=./image/keras.png>
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTUyMTU5ODQ2NCwtMzk3MjcxNzEsODYzMz
IyOTcxLC0xMjY5ODY1MzkzLDExNDg3NzIwMjEsMjAzNzM2OTM5
MiwxMzM2ODYyMzM5LDE2MTIwMTU2MjIsLTMwNjI1MDc1MywtNz
M4OTYwODExLDE1MTYyMjY4ODAsLTEzNjU1NDQyLDI1NzkxMjIx
NywtOTc3NTI3MjIxLC0xODcwMzM4ODMzLC0yMDA3MzgyOTc2LC
02MDY2NzEyNDIsLTM2NTE1MTAzM119
-->